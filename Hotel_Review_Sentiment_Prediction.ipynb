{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Libraries and create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des modules\n",
    "import time\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
    "\n",
    "#create Spark session\n",
    "appName = \"Sentiment Analysis in Spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data file into Spark dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/C:/Jupyter/tripadvisor_hotel_reviews.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cc6e61f0bb09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./tripadvisor_hotel_reviews.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/C:/Jupyter/tripadvisor_hotel_reviews.csv"
     ]
    }
   ],
   "source": [
    "#Specifiez votre path\n",
    "df= spark.read.csv('./tripadvisor_hotel_reviews.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic visualization of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20491"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Rating: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|summary|              Review|            Rating|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|               20491|             20491|\n",
      "|   mean|                null| 3.952222927138744|\n",
      "| stddev|                null|1.2330297776950543|\n",
      "|    min|1 best hotels new...|                 1|\n",
      "|    25%|                null|                 3|\n",
      "|    50%|                null|                 4|\n",
      "|    75%|                null|                 5|\n",
      "|    max|zero hip mid-bost...|                 5|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|              Review|Rating|\n",
      "+--------------------+------+\n",
      "|nice hotel expens...|     4|\n",
      "|ok nothing specia...|     2|\n",
      "|nice rooms not 4*...|     3|\n",
      "|unique, great sta...|     5|\n",
      "|great stay great ...|     5|\n",
      "|love monaco staff...|     5|\n",
      "|cozy stay rainy c...|     5|\n",
      "|excellent staff, ...|     4|\n",
      "|hotel stayed hote...|     5|\n",
      "|excellent stayed ...|     5|\n",
      "|poor value stayed...|     2|\n",
      "|nice value seattl...|     4|\n",
      "|nice hotel good l...|     4|\n",
      "|nice hotel not ni...|     3|\n",
      "|great hotel night...|     4|\n",
      "|horrible customer...|     1|\n",
      "|disappointed say ...|     2|\n",
      "|fantastic stay mo...|     5|\n",
      "|good choice hotel...|     5|\n",
      "|hmmmmm say really...|     3|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "#si Rating = 5,4,3 le rendre dans la table égale a 1\n",
    "#si Rating = 0,1,2 le rendre dans la table égale a 0\n",
    "\n",
    "df = df.withColumn('Rating', regexp_replace('Rating', '0', '0'))\n",
    "df = df.withColumn('Rating', regexp_replace('Rating', '1', '0'))\n",
    "df = df.withColumn('Rating', regexp_replace('Rating', '2', '0'))\n",
    "\n",
    "df = df.withColumn('Rating', regexp_replace('Rating', '5', '1'))\n",
    "df = df.withColumn('Rating', regexp_replace('Rating', '4', '1'))\n",
    "df = df.withColumn('Rating', regexp_replace('Rating', '3', '1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|              Review|Rating|\n",
      "+--------------------+------+\n",
      "|nice hotel expens...|     1|\n",
      "|ok nothing specia...|     0|\n",
      "|nice rooms not 4*...|     1|\n",
      "|unique, great sta...|     1|\n",
      "|great stay great ...|     1|\n",
      "|love monaco staff...|     1|\n",
      "|cozy stay rainy c...|     1|\n",
      "|excellent staff, ...|     1|\n",
      "|hotel stayed hote...|     1|\n",
      "|excellent stayed ...|     1|\n",
      "|poor value stayed...|     0|\n",
      "|nice value seattl...|     1|\n",
      "|nice hotel good l...|     1|\n",
      "|nice hotel not ni...|     1|\n",
      "|great hotel night...|     1|\n",
      "|horrible customer...|     0|\n",
      "|disappointed say ...|     0|\n",
      "|fantastic stay mo...|     1|\n",
      "|good choice hotel...|     1|\n",
      "|hmmmmm say really...|     1|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              Review|label|\n",
      "+--------------------+-----+\n",
      "|nice hotel expens...|    1|\n",
      "|ok nothing specia...|    0|\n",
      "|nice rooms not 4*...|    1|\n",
      "|unique, great sta...|    1|\n",
      "|great stay great ...|    1|\n",
      "|love monaco staff...|    1|\n",
      "|cozy stay rainy c...|    1|\n",
      "|excellent staff, ...|    1|\n",
      "|hotel stayed hote...|    1|\n",
      "|excellent stayed ...|    1|\n",
      "|poor value stayed...|    0|\n",
      "|nice value seattl...|    1|\n",
      "|nice hotel good l...|    1|\n",
      "|nice hotel not ni...|    1|\n",
      "|great hotel night...|    1|\n",
      "|horrible customer...|    0|\n",
      "|disappointed say ...|    0|\n",
      "|fantastic stay mo...|    1|\n",
      "|good choice hotel...|    1|\n",
      "|hmmmmm say really...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = df.select(\"Review\", col(\"Rating\").cast(\"Int\").alias(\"label\"))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Review: string, label: int]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data into training and testing data¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de TRAINING DATA 14245 ; Nombre de TEST DATA 6246\n"
     ]
    }
   ],
   "source": [
    "#diviser le dataset en 70% trainingdata et 30% testdata\n",
    "dividedData = data.randomSplit([0.7, 0.3]) \n",
    "trainingData = dividedData[0] \n",
    "testingData = dividedData[1] \n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print (\"Nombre de TRAINING DATA\", train_rows, \"; Nombre de TEST DATA\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning and Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separattion en utilisant tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Review\", outputCol=\"Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des stop words(anglais) \n",
    "#MeaningfulWords pour dire qu'on prend que les mots significatifs\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"MeaningfulWords\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertion des mots en numéro\n",
    "\n",
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"rawFeatures\", numFeatures=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IDF (Inverse Document Frequency)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation du piepline\n",
    "pipeline = Pipeline(stages=[tokenizer,swr, hashTF,idf])\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|              Review|label|               Words|     MeaningfulWords|         rawFeatures|            features|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|1 spot location f...|    1|[1, spot, locatio...|[1, spot, locatio...|(300,[6,7,11,13,1...|(300,[6,7,11,13,1...|\n",
      "|1 star best locat...|    0|[1, star, best, l...|[1, star, best, l...|(300,[0,4,16,27,3...|(300,[0,4,16,27,3...|\n",
      "|10 points, 10 poi...|    1|[10, points,, 10,...|[10, points,, 10,...|(300,[16,44,54,58...|(300,[16,44,54,58...|\n",
      "|10/10 impressed, ...|    1|[10/10, impressed...|[10/10, impressed...|(300,[0,1,4,9,16,...|(300,[0,1,4,9,16,...|\n",
      "|100 perfect husba...|    1|[100, perfect, hu...|[100, perfect, hu...|(300,[0,3,4,6,8,1...|(300,[0,3,4,6,8,1...|\n",
      "|100 stays 18 mont...|    1|[100, stays, 18, ...|[100, stays, 18, ...|(300,[0,4,6,27,33...|(300,[0,4,6,27,33...|\n",
      "|14.50 usd charge ...|    0|[14.50, usd, char...|[14.50, usd, char...|(300,[1,3,7,13,22...|(300,[1,3,7,13,22...|\n",
      "|19 yr old male op...|    1|[19, yr, old, mal...|[19, yr, old, mal...|(300,[0,3,4,5,6,8...|(300,[0,3,4,5,6,8...|\n",
      "|2 night pre cruis...|    1|[2, night, pre, c...|[2, night, pre, c...|(300,[1,4,5,6,8,1...|(300,[1,4,5,6,8,1...|\n",
      "|2 thumbs, pleased...|    1|[2, thumbs,, plea...|[2, thumbs,, plea...|(300,[3,4,9,12,15...|(300,[3,4,9,12,15...|\n",
      "|2 weeks fun sun, ...|    1|[2, weeks, fun, s...|[2, weeks, fun, s...|(300,[0,1,2,3,6,7...|(300,[0,1,2,3,6,7...|\n",
      "|2 weeks paradise ...|    1|[2, weeks, paradi...|[2, weeks, paradi...|(300,[4,5,10,11,1...|(300,[4,5,10,11,1...|\n",
      "|2 weeks paradise ...|    1|[2, weeks, paradi...|[2, weeks, paradi...|(300,[2,4,5,29,30...|(300,[2,4,5,29,30...|\n",
      "|2.5 stars masqera...|    0|[2.5, stars, masq...|[2.5, stars, masq...|(300,[0,3,5,9,12,...|(300,[0,3,5,9,12,...|\n",
      "|2nd time glad wen...|    1|[2nd, time, glad,...|[2nd, time, glad,...|(300,[4,10,13,15,...|(300,[4,10,13,15,...|\n",
      "|2nd time-loved, h...|    1|[2nd, time-loved,...|[2nd, time-loved,...|(300,[1,4,8,12,15...|(300,[1,4,8,12,15...|\n",
      "|2nd trip paradisu...|    1|[2nd, trip, parad...|[2nd, trip, parad...|(300,[0,3,8,10,11...|(300,[0,3,8,10,11...|\n",
      "|3 pleasant days s...|    1|[3, pleasant, day...|[3, pleasant, day...|(300,[0,1,4,10,12...|(300,[0,1,4,10,12...|\n",
      "|3 star hotel poor...|    0|[3, star, hotel, ...|[3, star, hotel, ...|(300,[0,5,12,16,2...|(300,[0,5,12,16,2...|\n",
      "|3 star hotel star...|    0|[3, star, hotel, ...|[3, star, hotel, ...|(300,[4,25,69,70,...|(300,[4,25,69,70,...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#transformation sur dataframe\n",
    "data=model.transform(trainingData)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(\"Words\").drop(\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fd = data.select(\"label\",\"features\").rdd.map(parsePoint)\n",
    "#fd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin du training\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "\n",
    "# Fit the model\n",
    "t0 = time.time()\n",
    "lsvcModel = lsvc.fit(data)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Fin du training\")\n",
    "#https://spark.apache.org/docs/latest/ml-classification-regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(testingData)\n",
    "test=model.transform(testingData).drop(\"Words\").drop(\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = time.time()\n",
    "predictions= lsvcModel.transform(test)\n",
    "t3 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|              Review|label|     MeaningfulWords|            features|       rawPrediction|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|100 star rating, ...|    1|[100, star, ratin...|(300,[0,4,13,16,2...|[-1.1260419750497...|       1.0|\n",
      "|15 group lovely t...|    1|[15, group, lovel...|(300,[4,11,58,66,...|[-0.7300255947919...|       1.0|\n",
      "|1925 elegance opu...|    1|[1925, elegance, ...|(300,[0,13,16,33,...|[-0.7964480115357...|       1.0|\n",
      "|2 star hotel 4 st...|    0|[2, star, hotel, ...|(300,[19,24,30,70...|[-0.4298052551710...|       1.0|\n",
      "|3 stays feel comp...|    1|[3, stays, feel, ...|(300,[0,1,2,4,6,1...|[-2.7945766636415...|       1.0|\n",
      "|3rd time stayed v...|    1|[3rd, time, staye...|(300,[4,17,20,28,...|[-0.9033231195475...|       1.0|\n",
      "|5 star service gr...|    1|[5, star, service...|(300,[0,4,7,12,13...|[-3.1057227256597...|       1.0|\n",
      "|5 star service ho...|    1|[5, star, service...|(300,[0,1,3,4,5,1...|[-0.7523320646454...|       1.0|\n",
      "|a++ bavaro prince...|    1|[a++, bavaro, pri...|(300,[12,24,26,30...|[-0.7659347589119...|       1.0|\n",
      "|ab fab, thinking ...|    1|[ab, fab,, thinki...|(300,[0,11,13,16,...|[-1.3236855280639...|       1.0|\n",
      "|absolute paradise...|    1|[absolute, paradi...|(300,[0,4,8,12,15...|[-0.5072436138802...|       1.0|\n",
      "|absolutely charmi...|    1|[absolutely, char...|(300,[0,1,4,5,6,7...|[-0.9644573600263...|       1.0|\n",
      "|absolutely dreadf...|    0|[absolutely, drea...|(300,[4,7,11,12,1...|[-0.4675486455873...|       1.0|\n",
      "|absolutely fantas...|    1|[absolutely, fant...|(300,[9,16,24,25,...|[-0.0130206971567...|       1.0|\n",
      "|absolutely loved ...|    1|[absolutely, love...|(300,[0,3,4,12,14...|[-1.7731590862726...|       1.0|\n",
      "|absolutely place ...|    1|[absolutely, plac...|(300,[7,20,22,24,...|[-0.8833601889386...|       1.0|\n",
      "|absolutely wonder...|    1|[absolutely, wond...|(300,[3,4,11,16,2...|[-1.3562171707146...|       1.0|\n",
      "|abysmal service e...|    0|[abysmal, service...|(300,[0,2,3,8,16,...|[-1.0711412041914...|       1.0|\n",
      "|acceptable space ...|    1|[acceptable, spac...|(300,[4,6,8,9,22,...|[-0.7080454469220...|       1.0|\n",
      "|ace grunge lives ...|    0|[ace, grunge, liv...|(300,[27,30,44,97...|[-0.3521039475682...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps de training: 7.156293s; Temps de prediction: 0.063727s\n"
     ]
    }
   ],
   "source": [
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t3-t2\n",
    "\n",
    "print(\"Temps de training: %fs; Temps de prediction: %fs\" % (time_linear_train, time_linear_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate the accuracy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.856227985910983\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Precision = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NaiveBayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin du training\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import NaiveBayesModel\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes()\n",
    "#utilisation du naive bayes sur training data après prétraitement\n",
    "\n",
    "t0 = time.time()\n",
    "NBmodel = nb.fit(data)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Fin du training\")\n",
    "#https://spark.apache.org/docs/latest/ml-classification-regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = time.time()\n",
    "NB_predictions = NBmodel.transform(test)\n",
    "t3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|              Review|label|     MeaningfulWords|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|100 star rating, ...|    1|[100, star, ratin...|(300,[0,4,13,16,2...|[-455.96914057742...|[2.28367424401234...|       1.0|\n",
      "|15 group lovely t...|    1|[15, group, lovel...|(300,[4,11,58,66,...|[-100.87373408807...|[0.02286460027457...|       1.0|\n",
      "|1925 elegance opu...|    1|[1925, elegance, ...|(300,[0,13,16,33,...|[-338.80073009137...|[0.01762810776677...|       1.0|\n",
      "|2 star hotel 4 st...|    0|[2, star, hotel, ...|(300,[19,24,30,70...|[-232.02906507727...|[0.60556180812210...|       0.0|\n",
      "|3 stays feel comp...|    1|[3, stays, feel, ...|(300,[0,1,2,4,6,1...|[-785.69616366993...|[5.78070194370217...|       1.0|\n",
      "|3rd time stayed v...|    1|[3rd, time, staye...|(300,[4,17,20,28,...|[-598.00869099722...|[0.00397059208997...|       1.0|\n",
      "|5 star service gr...|    1|[5, star, service...|(300,[0,4,7,12,13...|[-1323.1372506215...|[2.89213984112015...|       1.0|\n",
      "|5 star service ho...|    1|[5, star, service...|(300,[0,1,3,4,5,1...|[-974.81079544460...|[0.95997122827208...|       0.0|\n",
      "|a++ bavaro prince...|    1|[a++, bavaro, pri...|(300,[12,24,26,30...|[-293.09971070069...|[0.04544587387461...|       1.0|\n",
      "|ab fab, thinking ...|    1|[ab, fab,, thinki...|(300,[0,11,13,16,...|[-434.41948194739...|[5.75434408023151...|       1.0|\n",
      "|absolute paradise...|    1|[absolute, paradi...|(300,[0,4,8,12,15...|[-899.64891203751...|[0.14877233450257...|       1.0|\n",
      "|absolutely charmi...|    1|[absolutely, char...|(300,[0,1,4,5,6,7...|[-1274.1726689102...|[0.29219244748695...|       1.0|\n",
      "|absolutely dreadf...|    0|[absolutely, drea...|(300,[4,7,11,12,1...|[-566.09919130798...|[0.72185592246769...|       0.0|\n",
      "|absolutely fantas...|    1|[absolutely, fant...|(300,[9,16,24,25,...|[-419.19540971615...|[0.64513319108136...|       0.0|\n",
      "|absolutely loved ...|    1|[absolutely, love...|(300,[0,3,4,12,14...|[-1045.3503940473...|[1.14016745132558...|       1.0|\n",
      "|absolutely place ...|    1|[absolutely, plac...|(300,[7,20,22,24,...|[-205.29036491181...|[0.03238997764181...|       1.0|\n",
      "|absolutely wonder...|    1|[absolutely, wond...|(300,[3,4,11,16,2...|[-389.30611188559...|[6.28789277275278...|       1.0|\n",
      "|abysmal service e...|    0|[abysmal, service...|(300,[0,2,3,8,16,...|[-1003.5911109372...|[0.99811964637765...|       0.0|\n",
      "|acceptable space ...|    1|[acceptable, spac...|(300,[4,6,8,9,22,...|[-451.11007027953...|[0.18395550290396...|       1.0|\n",
      "|ace grunge lives ...|    0|[ace, grunge, liv...|(300,[27,30,44,97...|[-111.41538492285...|[0.20354602629583...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin du training 3.857038s; Temps de prediction: 0.075362s\n"
     ]
    }
   ],
   "source": [
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t3-t2\n",
    "\n",
    "print(\"Fin du training %fs; Temps de prediction: %fs\" % (time_linear_train, time_linear_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate the accuracy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8603906500160102\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(NB_predictions)\n",
    "print(\"Precision = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RANDOM FOREST Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training is done\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "\n",
    "t0 = time.time()\n",
    "rfModel = rf.fit(data)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"training is done\")\n",
    "#https://spark.apache.org/docs/latest/ml-classification-regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = time.time()\n",
    "RF_predictions = rfModel.transform(test)\n",
    "t3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|              Review|label|     MeaningfulWords|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|100 star rating, ...|    1|[100, star, ratin...|(300,[0,4,13,16,2...|[3.12752129263693...|[0.15637606463184...|       1.0|\n",
      "|15 group lovely t...|    1|[15, group, lovel...|(300,[4,11,58,66,...|[2.60960770343154...|[0.13048038517157...|       1.0|\n",
      "|1925 elegance opu...|    1|[1925, elegance, ...|(300,[0,13,16,33,...|[3.06494339439460...|[0.15324716971973...|       1.0|\n",
      "|2 star hotel 4 st...|    0|[2, star, hotel, ...|(300,[19,24,30,70...|[3.38375617359375...|[0.16918780867968...|       1.0|\n",
      "|3 stays feel comp...|    1|[3, stays, feel, ...|(300,[0,1,2,4,6,1...|[2.08256511352616...|[0.10412825567630...|       1.0|\n",
      "|3rd time stayed v...|    1|[3rd, time, staye...|(300,[4,17,20,28,...|[2.81059641480475...|[0.14052982074023...|       1.0|\n",
      "|5 star service gr...|    1|[5, star, service...|(300,[0,4,7,12,13...|[3.72342558302121...|[0.18617127915106...|       1.0|\n",
      "|5 star service ho...|    1|[5, star, service...|(300,[0,1,3,4,5,1...|[5.23755477828178...|[0.26187773891408...|       1.0|\n",
      "|a++ bavaro prince...|    1|[a++, bavaro, pri...|(300,[12,24,26,30...|[2.54885397542013...|[0.12744269877100...|       1.0|\n",
      "|ab fab, thinking ...|    1|[ab, fab,, thinki...|(300,[0,11,13,16,...|[2.66038548210892...|[0.13301927410544...|       1.0|\n",
      "|absolute paradise...|    1|[absolute, paradi...|(300,[0,4,8,12,15...|[3.64954827242866...|[0.18247741362143...|       1.0|\n",
      "|absolutely charmi...|    1|[absolutely, char...|(300,[0,1,4,5,6,7...|[5.29998466535481...|[0.26499923326774...|       1.0|\n",
      "|absolutely dreadf...|    0|[absolutely, drea...|(300,[4,7,11,12,1...|[4.24495177888227...|[0.21224758894411...|       1.0|\n",
      "|absolutely fantas...|    1|[absolutely, fant...|(300,[9,16,24,25,...|[3.41143004066136...|[0.17057150203306...|       1.0|\n",
      "|absolutely loved ...|    1|[absolutely, love...|(300,[0,3,4,12,14...|[2.25213260139199...|[0.11260663006959...|       1.0|\n",
      "|absolutely place ...|    1|[absolutely, plac...|(300,[7,20,22,24,...|[2.86140658211837...|[0.14307032910591...|       1.0|\n",
      "|absolutely wonder...|    1|[absolutely, wond...|(300,[3,4,11,16,2...|[3.05961102268963...|[0.15298055113448...|       1.0|\n",
      "|abysmal service e...|    0|[abysmal, service...|(300,[0,2,3,8,16,...|[4.23157406985197...|[0.21157870349259...|       1.0|\n",
      "|acceptable space ...|    1|[acceptable, spac...|(300,[4,6,8,9,22,...|[3.44474668932894...|[0.17223733446644...|       1.0|\n",
      "|ace grunge lives ...|    0|[ace, grunge, liv...|(300,[27,30,44,97...|[3.37401175024741...|[0.16870058751237...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF_predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps de training: 11.625375s; Temps de prediction: 0.166021s\n"
     ]
    }
   ],
   "source": [
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t3-t2\n",
    "\n",
    "print(\"Temps de training: %fs; Temps de prediction: %fs\" % (time_linear_train, time_linear_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate the accuracy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8395773294908742\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(RF_predictions)\n",
    "print(\"Precision = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
